# VerbaTexta: Pipeline de An√°lisis L√©xico y Sem√°ntico para Cualquier Texto

Este proyecto pone a disposici√≥n un script de Python que, usando t√©cnicas de Procesamiento de Lenguaje Natural (PLN) y embeddings de palabras, permite:
- Tokenizar y limpiar cualquier texto plano.
- Eliminar stopwords y lematizar tokens basados en POS tagging.
- Generar listas de t√©rminos candidatos mediante POS tagging.
- Sugerir t√©rminos sem√°nticamente relacionados usando un modelo preentrenado (FastText).
- Permitir una selecci√≥n manual de t√©rminos y luego calcular su frecuencia y concordancias.


## Funcionalidades Principales

* Carga y preprocesa el texto de la novela.
* Realiza lematizaci√≥n asistida por etiquetado gramatical (POS tagging).
* Genera una lista de t√©rminos candidatos (sustantivos y adjetivos frecuentes) mediante POS tagging.
* Carga un modelo FastText preentrenado para usar embeddings de palabras sin necesidad de entrenamiento local.
* Utiliza el modelo FastText y un conjunto de palabras "semilla" (definibles por el usuario) para sugerir t√©rminos sem√°nticamente relacionados con un concepto (ej. "paisaje y naturaleza").
* Permite al usuario ingresar interactivamente una lista final de t√©rminos a analizar, bas√°ndose en las sugerencias del POS tagging y FastText.
* Calcula la frecuencia de los t√©rminos seleccionados por el usuario.
* Muestra concordancias (KWIC - Keyword in Context) para los t√©rminos seleccionados.

## Requisitos

1.  **Python 3:** Se recomienda Python 3.7 o superior.
2.  **Bibliotecas de Python:**
    * NLTK (`nltk`): Para tokenizaci√≥n, POS tagging, stopwords, lematizaci√≥n, etc.
    * Gensim (`gensim`): Para entrenar el modelo Word2Vec.
    Puedes instalarlas usando pip:
    ```bash
    pip3 install nltk gensim
    ```
3.  **Recursos de NLTK:**
    El script intentar√° usar los siguientes recursos de NLTK. Si no est√°n presentes, NLTK podr√≠a intentar descargarlos o dar un error. Es recomendable descargarlos una vez ejecutando Python y luego los siguientes comandos:
    ```python
    import nltk
    nltk.download('punkt')         # Para tokenizaci√≥n de frases y palabras
    nltk.download('stopwords')     # Para la lista de palabras vac√≠as (stopwords)
    nltk.download('averaged_perceptron_tagger') # Para el POS tagging
    nltk.download('wordnet')       # Para el WordNetLemmatizer
    nltk.download('omw-1.4')       # Open Multilingual Wordnet, necesario para WordNetLemmatizer
    ```
4.  **Archivo de Texto de la Novela:**
    * El script espera un archivo de texto plano (`.txt`) de "The Purple Land".
    * Actualmente, la ruta al archivo est√° codificada directamente en el script en la variable `file_path`. Deber√°s modificar esta l√≠nea para que apunte a la ubicaci√≥n de tu archivo:
        ```python
        file_path = '/ruta/a/tu/archivo/purple_land_hudson.txt'
        ```

## C√≥mo Ejecutar el Script

1.  Aseg√∫rate de tener todos los requisitos instalados.
2.  Guarda el script de Python (ej. `analizador_paisaje_hudson.py`) y el archivo de texto de la novela en tu computadora.
3.  Modifica la variable `file_path` dentro del script para que apunte a tu archivo de texto.
4.  Abre una terminal o l√≠nea de comandos.
5.  Navega hasta el directorio donde guardaste el script.
6.  Ejecuta el script con:
    ```bash
    python3 analizador_paisaje_hudson.py
    ```
7.  El script te guiar√° a trav√©s de varios pasos, imprimiendo informaci√≥n y listas de candidatos. Presta atenci√≥n a los prompts interactivos:
    * **Selecci√≥n de Palabras Semilla para Word2Vec (Paso 3.5):** Se te presentar√° una lista de palabras semilla por defecto para el concepto de "paisaje/naturaleza". Podr√°s elegir usar esta lista o ingresar la tuya propia (separada por comas).
    * **Selecci√≥n Final de T√©rminos para An√°lisis (Paso 4):** Despu√©s de ver los candidatos del POS tagging (Paso 3) y las sugerencias sem√°nticas de Word2Vec (Paso 3.5), se te pedir√° que ingreses la lista final de t√©rminos (separados por comas) que deseas analizar en detalle.

## Flujo de Trabajo del Script

El script realiza los siguientes pasos principales:

1.  **Paso 1: Carga del Archivo de Texto:** Lee el contenido de la novela desde el archivo especificado.
2.  **Paso 2: Preprocesamiento B√°sico General:**
    * Tokeniza el texto en palabras.
    * Convierte todos los tokens a min√∫sculas.
    * Elimina puntuaci√≥n y stopwords para crear una lista de `tokens_processed`.
    * Realiza POS tagging sobre `tokens_processed` y luego lematiza estos tokens para crear `tokens_lemmatized`. Esta lista lematizada es la base para los an√°lisis de frecuencia y concordancias.
    * Crea un objeto `nltk.Text` a partir de `tokens_lemmatized` para las concordancias.
3.  **Paso 2.1: Conteo de Frecuencia General:** Calcula y muestra las palabras m√°s frecuentes en todo el texto (basado en `tokens_lemmatized`).
4.  **Paso 3: Generaci√≥n de Candidatos por POS Tagging:**
    * Realiza POS tagging sobre `tokens_processed` (tokens limpios, pero no lematizados).
    * Filtra para quedarse con sustantivos y adjetivos.
    * Muestra los 50 sustantivos/adjetivos m√°s frecuentes como una primera lista de candidatos (en sus formas originales, no lematizadas).
5.  **Paso 3.5: Filtrado Sem√°ntico Local con Word Embeddings (Gensim):**
    * Prepara el corpus para Word2Vec (oraciones tokenizadas del texto original, en min√∫sculas, solo palabras alfab√©ticas).
    * Entrena un modelo Word2Vec sobre este corpus.
    * Permite al usuario definir o usar una lista por defecto de **palabras semilla** representativas del concepto "paisaje/naturaleza".
    * Valida qu√© palabras semilla existen en el vocabulario del modelo Word2Vec.
    * Toma los N t√©rminos m√°s frecuentes del corpus de Word2Vec como candidatos a filtrar.
    * Calcula la similitud (usando el modelo Word2Vec) entre cada t√©rmino candidato y las palabras semilla v√°lidas.
    * Presenta una lista de t√©rminos que superan un umbral de similitud predefinido.
6.  **Paso 4: Selecci√≥n Interactiva de T√©rminos por el Usuario:**
    * El usuario revisa las listas de candidatos del Paso 3 y las sugerencias del Paso 3.5.
    * Ingresa una lista final de t√©rminos de inter√©s, separados por comas.
    * Estos t√©rminos ingresados son luego lematizados por el script.
7.  **Paso 5: Conteo de Frecuencia de T√©rminos Seleccionados:** Calcula y muestra la frecuencia de cada t√©rmino seleccionado por el usuario (ya lematizado) dentro del corpus lematizado (`tokens_lemmatized`).
8.  **Paso 6: An√°lisis de Concordancias:** Muestra las l√≠neas de contexto donde aparecen los t√©rminos seleccionados por el usuario (lematizados) dentro del texto lematizado (`nltk_text`).

## Notas sobre el Modelo Word2Vec (Paso 3.5)

* El modelo Word2Vec se **entrena desde cero cada vez que se ejecuta el script**. Para corpus muy grandes, esto podr√≠a ser lento. Para una sola novela, deber√≠a ser razonablemente r√°pido.
* **Par√°metros Clave para Experimentaci√≥n:**
    * `vector_dim`: Dimensionalidad de los vectores de palabras (ej. 50-150 para un corpus peque√±o).
    * `window_size`: Tama√±o de la ventana de contexto (ej. 3-7).
    * `min_word_count`: Frecuencia m√≠nima para que una palabra sea incluida en el vocabulario del modelo (ej. 2-5). Un valor bajo es importante para un corpus de una sola novela.
    * `sg_param`: Algoritmo de entrenamiento (0 para CBOW, 1 para Skip-gram).
    * `seed_words_nature_final`: La lista de palabras que definen tu concepto de "paisaje/naturaleza".
    * `similarity_threshold`: El umbral para considerar un t√©rmino como sem√°nticamente similar (ej. 0.5 - 0.7). Este valor requiere bastante experimentaci√≥n.
* La calidad de las sugerencias sem√°nticas depender√° de estos par√°metros y de la especificidad del lenguaje en la novela.

## Personalizaci√≥n

* **Archivo de Texto:** Cambia la variable `file_path`.
* **Palabras Semilla:** Modifica la lista `default_seed_words_nature` en el script o proporciona tu propia lista interactivamente.
* **Par√°metros de Word2Vec y Umbral de Similitud:** Ajusta estos valores directamente en la secci√≥n del "Paso 3.5" para refinar los resultados del filtrado sem√°ntico.

---

_Este README fue generado el 24 de mayo de 2025._
# VerbaTexta Word-Embeddings Pipeline

Este proyecto es una **herramienta de l√≠nea de comandos** en Python para:

1. **Tokenizaci√≥n y limpieza** del texto fuente  
2. **Eliminaci√≥n de stopwords** (ingl√©s)  
3. **Lematizaci√≥n** guiada por POS-tagging (NLTK + WordNet)  
4. **Generaci√≥n de candidatos l√©xicos** mediante POS-tagging (sustantivos y adjetivos frecuentes)  
5. **Filtrado sem√°ntico offline** con **FastText preentrenado**  
6. **Selecci√≥n manual** de t√©rminos finales y  
7. **Conteo de frecuencia** + **concordancias** contextualizadas  

---

## üìÇ Estructura

```
verbatexta/
‚îú‚îÄ‚îÄ verbatexta_wordembeddings         # Script principal (Python)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ wiki-news-300d-1M.vec         # FastText preentrenado
‚îî‚îÄ‚îÄ README.md                         # Este archivo
```

---

## ‚öôÔ∏è Requisitos

- **Python 3.8+**  
- Espacio en disco: **~1 GB** para el `.vec` de FastText  
- Red: s√≥lo para la descarga original de FastText; el an√°lisis es **offline**.

### Librer√≠as Python

Instal√° las dependencias:

```bash
pip install nltk gensim matplotlib
```

### Recursos de NLTK

Abr√≠ un int√©rprete Python una vez y ejecut√°:

```python
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
```

(Despu√©s pod√©s comentar o quitar las l√≠neas `nltk.download` del script.)

---

## üì• Descargar FastText preentrenado

1. Descarg√° **wiki-news-300d-1M.vec.zip** desde  
   https://fasttext.cc/docs/en/english-vectors.html  
2. Mu√©velo a la carpeta `models/` de tu proyecto y descomprimilo:  
   ```bash
   mkdir -p ~/Desktop/verbatexta/models
   mv ~/Downloads/wiki-news-300d-1M.vec.zip ~/Desktop/verbatexta/models/
   cd ~/Desktop/verbatexta/models
   unzip wiki-news-300d-1M.vec.zip
   ```
3. Verific√° que exista  
   `~/Desktop/verbatexta/models/wiki-news-300d-1M.vec`

---

## üöÄ C√≥mo ejecutar

Desde la ra√≠z del proyecto:

```bash
python verbatexta_wordembeddings
```

El script recorrer√° estos pasos:

1.  **Carga** del archivo de texto (`raw_text`) en `file_path`.  
2.  **Tokenizaci√≥n** en palabras y oraciones.  
3.  **Lowercasing** y filtrado de tokens alfab√©ticos.  
4.  **Stopwords**: eliminaci√≥n seg√∫n NLTK.  
5.  **Lematizaci√≥n**:  
   - POS-tagging sobre los tokens limpios  
   - Lematizaci√≥n con `WordNetLemmatizer` y categor√≠as de WordNet  
6.  **Conteo de frecuencia** de todos los lemas (Top 20 por pantalla).  
7.  **POS-candidates**: impresi√≥n de los 50 t√©rminos m√°s frecuentes etiquetados como sustantivos o adjetivos (forma original, no lematizada).  
8.  **Paso 3.5 ‚Äì Filtrado sem√°ntico**:  
   - Carga del modelo FastText  
   - Definici√≥n de una lista de **semillas** por defecto (`tree, river, mountain‚Ä¶`) o personalizadas  
   - C√°lculo de similitud cosine entre cada t√©rmino candidato (Top 200) y las semillas  
   - Selecci√≥n de t√©rminos cuya similitud m√°xima supere un **umbral** (por defecto 0.75)  
   - Impresi√≥n de un **√∫nico diagn√≥stico** (Top 20 t√©rminos + estad√≠sticas: m√≠nima, m√°xima, promedio)  
   - Impresi√≥n de la **lista final** de sugerencias sem√°nticas  
9.  **Selecci√≥n manual**: el usuario puede tomar esas sugerencias o escribir cualquier otra lista de t√©rminos.  
10. **Lematizaci√≥n** de la lista manual y **conteo de frecuencia** sobre el texto lematizado.  
11. **Concordancias** (10 l√≠neas de contexto) de cada t√©rmino seleccionado, sobre el texto lematizado (`nltk_text`).

---

## üîß Personalizaci√≥n

- **Ruta de texto**: cambia `file_path` al camino de tu propio `.txt`.  
- **Ruta FastText**: modifica `fasttext_path` para apuntar a tu archivo `.vec`.  
- **Semillas y umbral**: puedes ajustar la lista `default_seed_words_nature` y el valor `similarity_threshold` en pantalla.  
- **Pool de candidatos**: en lugar de Top 200, cambia el par√°metro de `most_common(200)` seg√∫n tu corpus.

---

## üìñ Lecturas recomendadas

- Bojanowski et al. (2017). *Enriching Word Vectors with Subword Information* (FastText)  
- Mikolov et al. (2013). *Efficient Estimation of Word Representations in Vector Space* (Word2Vec)  
- Bird, Klein & Loper (2009). *Natural Language Processing with Python* (NLTK)

---

_Este README fue generado el 24 de mayo de 2025._