import nltk
# --- NLTK Downloads: Ejecutar una vez en consola de Python si es necesario, luego comentar ---
# nltk.download('punkt')
# # nltk.download('punkt_tab') # 'punkt' es el principal
# nltk.download('stopwords')
# nltk.download('averaged_perceptron_tagger')
# # nltk.download('averaged_perceptron_tagger_eng') # 'averaged_perceptron_tagger' es suficiente
# nltk.download('wordnet')
# nltk.download('omw-1.4') # Necesario para WordNetLemmatizer en algunos idiomas/versiones

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.text import Text # Para concordancias
import string # Para manejar puntuación
from collections import Counter
from nltk.corpus import wordnet
from gensim.models import KeyedVectors  # Importar carga de FastText preentrenado

# import os # Si fueras a usar variables de entorno para claves API

# --- Función para mapear etiquetas POS de NLTK a WordNet ---
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Por defecto, sustantivo

# --- PASO 1: CARGAR EL ARCHIVO DE TEXTO ---
file_path = '/Users/juanaguerre/Downloads/purple_land_hudson.txt'
try:
    with open(file_path, 'r', encoding='utf-8') as file:
        raw_text = file.read()
except FileNotFoundError:
    print(f"Error: El archivo '{file_path}' no fue encontrado. Verifica la ruta.")
    exit()

# --- PASO 2: PREPROCESAMIENTO BÁSICO GENERAL ---
print("\n--- Iniciando Paso 2: Preprocesamiento Básico General ---")
tokens_originales = word_tokenize(raw_text)
tokens_lower = [token.lower() for token in tokens_originales]

tokens_no_punct_list = [token for token in tokens_lower if token.isalpha()]
stop_words_english = set(stopwords.words('english'))
print("\n--- DIAGNÓSTICO DE STOPWORDS ---")
print(f"Número de stopwords en NLTK (inglés): {len(stop_words_english)}")
if 'the' in stop_words_english and 'a' in stop_words_english and 'is' in stop_words_english:
    print("Diagnóstico: Stopwords comunes como 'the', 'a', 'is' SÍ están en la lista de NLTK.")
else:
    print("Diagnóstico ADVERTENCIA: Stopwords comunes como 'the', 'a', 'is' NO están en la lista. Problema con stopwords.")
tokens_processed = [token for token in tokens_no_punct_list if token not in stop_words_english] # Usado para POS Tagging en Paso 3

lemmatizer = WordNetLemmatizer()
# Lematización: Primero POS tag sobre tokens_processed, luego lematizar.
pos_tags_for_lemmatization = nltk.pos_tag(tokens_processed) ### MODIFICACIÓN: POS Tagging sobre tokens_processed
tokens_lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags_for_lemmatization]

# nltk_text para concordancias se basará en tokens_lemmatized
nltk_text = Text(tokens_lemmatized)

print(f"Número de tokens originales: {len(tokens_originales)}")
print(f"Número de tokens procesados (antes de lematizar, para referencia): {len(tokens_processed)}")
print(f"Número de tokens lematizados (para análisis principal): {len(tokens_lemmatized)}")
print("\nPrimeros 20 tokens lematizados:")
print(tokens_lemmatized[:20])

# --- PASO 2.1: CONTEO DE FRECUENCIA DE TODAS LAS PALABRAS LEMATIZADAS ---
print("\n--- Iniciando Paso 2.1: Frecuencia de Todas las Palabras Lematizadas ---")
word_counts = Counter(tokens_lemmatized) # Usar tokens lematizados para el conteo general
print("\n--- Frecuencia de Todas las Palabras Lematizadas (Top 20) ---")
for word, count in word_counts.most_common(20):
    print(f"{word}: {count}")

# --- PASO 3: GENERACIÓN DE CANDIDATOS DE TÉRMINOS (con POS Tagging, antes de lematizar) ---
print("\n--- Iniciando Paso 3: Generación de Candidatos con POS Tagging ---")
### MODIFICACIÓN: Aplicar POS Tagging sobre 'tokens_processed' (sin stopwords/puntuación, pero NO lematizados aún)
### Esto es para obtener candidatos en su forma más natural antes de que el usuario los seleccione y se lematicen.
tagged_tokens_for_candidates = nltk.pos_tag(tokens_processed)

pos_tags_of_interest = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']
candidate_terms_from_pos = []
for word, tag in tagged_tokens_for_candidates:
    if tag in pos_tags_of_interest:
        # word.isalpha() ya está implícito porque tokens_processed se basa en tokens_no_punct_list
        # stopwords ya fueron removidas para crear tokens_processed
        candidate_terms_from_pos.append(word)

candidate_counts = Counter(candidate_terms_from_pos)

print("\n--- Lista de Candidatos por POS Tagging (Top 50 más frecuentes, formas originales) ---")
print("REVISA ESTA LISTA. Los términos que elijas se lematizarán después.")
for term, count in candidate_counts.most_common(50):
    print(f"{term}: {count}")

# --- PASO 3.5: FILTRADO SEMÁNTICO LOCAL CON WORD EMBEDDINGS (GENSIM) ---
print("\n" + "="*50)
print("PASO 3.5: FILTRADO SEMÁNTICO LOCAL CON WORD EMBEDDINGS (GENSIM)")
print("="*50)

# --- CARGAR MODELO FASTTEXT PREENTRENADO ---
fasttext_path = '/Users/juanaguerre/Desktop/verbatexta/models/wiki-news-300d-1M.vec'  # Actualiza con la ruta real
print(f"Cargando FastText preentrenado desde {fasttext_path}...")
w2v_model = KeyedVectors.load_word2vec_format(fasttext_path, binary=False)
print("Modelo FastText cargado.")
# 3. Definir/Ingresar palabras semilla para "paisaje y naturaleza"
default_seed_words_nature = [
        'tree', 'river', 'mountain', 'forest', 'pampa', 'sky', 'water', 'plain',
        'grass', 'stone', 'wind', 'flower', 'sea', 'hill', 'valley', 'sun',
        'moon', 'stars', 'animal', 'bird', 'wood', 'field', 'soil', 'plant',
        'nature', 'landscape', 'ground', 'earth'
    ]
print("\nPalabras semilla por defecto para 'paisaje/naturaleza':")
print(", ".join(default_seed_words_nature))

use_default_seeds_prompt = input("¿Usar esta lista de semillas por defecto? (s/n, 's' por defecto): ").strip().lower()

seed_words_nature_final = []
if use_default_seeds_prompt == 'n':
    print("\nIngresa tus propias palabras semilla para 'paisaje/naturaleza', separadas por comas.")
    user_seed_input_string = input("Tus palabras semilla: ").strip()
    if user_seed_input_string:
        raw_seed_terms = user_seed_input_string.split(',')
        for term in raw_seed_terms:
            cleaned_term = term.strip().lower()
            if cleaned_term:
                seed_words_nature_final.append(cleaned_term)
    if not seed_words_nature_final: # Si el usuario ingresó 'n' pero no dio palabras
        print("No se ingresaron palabras semilla personalizadas. Usando la lista por defecto.")
        seed_words_nature_final = default_seed_words_nature
else: # Si el usuario presiona Enter o escribe 's' (o cualquier cosa que no sea 'n')
    seed_words_nature_final = default_seed_words_nature

print(f"Se usarán las siguientes palabras semilla para el análisis de similitud: {seed_words_nature_final}")

# Filtrar seed_words_nature_final para que solo contengan palabras que están en el vocabulario del modelo FastText
valid_seed_words = [word for word in seed_words_nature_final if word in w2v_model]

if not valid_seed_words:
    print("Advertencia: Ninguna de las palabras semilla seleccionadas se encuentra en el vocabulario del modelo FastText.")
    print("El filtrado semántico basado en similitud no producirá resultados.")
    # semantic_suggestions ya fue inicializada como [] al principio del Paso 3.5
else:
    print(f"Palabras semilla válidas en el modelo para 'paisaje/naturaleza': {valid_seed_words}")

    # 4. Tomar los términos candidatos para filtrar.
    # NOTA: Necesitas definir word2vec_corpus_cleaned antes de este bloque si no está definido.
    # Aquí se asume que word2vec_corpus_cleaned es una lista de listas de tokens (frases tokenizadas).
    # Si no tienes esta variable, deberás crearla a partir de tu corpus.
    # Por ejemplo:
    # word2vec_corpus_cleaned = [word_tokenize(sent.lower()) for sent in sent_tokenize(raw_text)]
    
    raw_sentences = sent_tokenize(raw_text.lower())
    word2vec_corpus = [word_tokenize(sentence) for sentence in raw_sentences]
    word2vec_corpus_cleaned = []
    for sentence_tokens in word2vec_corpus:
        cleaned_sentence = [token for token in sentence_tokens if token.isalpha()]
        if cleaned_sentence:
            word2vec_corpus_cleaned.append(cleaned_sentence)

    
    temp_counter_for_candidates = Counter()
    for sentence in word2vec_corpus_cleaned:
        temp_counter_for_candidates.update(sentence)
    most_frequent_unlemmatized = temp_counter_for_candidates.most_common(200)
            
    candidate_terms_for_wv = [
        word for word, count in most_frequent_unlemmatized
        if word not in stop_words_english
    ]
    print(f"Número de términos frecuentes (después de quitar stopwords) para el análisis de similitud FastText: {len(candidate_terms_for_wv)}")
    
    similarity_threshold = 0.75 # Experimenta con este valor
    print(f"\nFiltrando ~{len(candidate_terms_for_wv)} términos con umbral > {similarity_threshold}...")
    similarities_calculated = {}
    semantic_suggestions = [] 
    for term in candidate_terms_for_wv:
        if term in w2v_model:
            try:
                max_similarity = 0.0
                for seed in valid_seed_words:
                    sim = w2v_model.similarity(term, seed)
                    if sim > max_similarity:
                        max_similarity = sim
                similarities_calculated[term] = max_similarity
                if max_similarity > similarity_threshold:
                    if term not in valid_seed_words:
                        semantic_suggestions.append(term)
            except KeyError:
                pass

    # Consolidar y ordenar sugerencias únicas
    semantic_suggestions = sorted(dict.fromkeys(semantic_suggestions))

    # Mostrar diagnóstico una sola vez
    if similarities_calculated:
        sorted_similarities = sorted(similarities_calculated.items(), key=lambda item: item[1], reverse=True)
        print("\n--- DIAGNÓSTICO: Resumen de Similitudes Calculadas ---")
        print("Top 20 términos más similares (término: similitud):")
        for t, s_score in sorted_similarities[:20]:
            print(f"  {t}: {s_score:.4f}")
        scores = list(similarities_calculated.values())
        print(f"\nEstadísticas de similitud: mínima {min(scores):.4f}, máxima {max(scores):.4f}, promedio {sum(scores)/len(scores):.4f}")
    else:
        print("No se calcularon similitudes para ningún término candidato.")

    if semantic_suggestions:
        print(", ".join(semantic_suggestions))
    else:
        print("No se encontraron sugerencias semánticas con los parámetros actuales (o no había palabras semilla válidas).")

# --- PASO 4: ENTRADA INTERACTIVA DEL USUARIO PARA SELECCIONAR TÉRMINOS ---
print("\n" + "="*50)
print("PASO 4: SELECCIÓN INTERACTIVA DE TÉRMINOS")
print("="*50)
### MODIFICACIÓN: Actualizar el prompt para el usuario
print("Por favor, revisa la 'Lista de Candidatos por POS Tagging' (Paso 3) y los")
print("'Términos sugeridos por Word2Vec' (Paso 3.5) impresos arriba.")
print("Escribe los términos que consideres relevantes para tu análisis (se lematizarán después).")
input_prompt = "Ingresa los términos separados por comas (ej: pampa, monte, land, sky): "
user_input_string = input(input_prompt)

landscape_terms_from_user_input = []
if user_input_string.strip():
    raw_terms = user_input_string.split(',')
    for term in raw_terms:
        cleaned_term = term.strip().lower()
        if cleaned_term:
            landscape_terms_from_user_input.append(cleaned_term)

if not landscape_terms_from_user_input:
    print("No se ingresaron términos. El análisis de términos específicos se omitirá.")
    landscape_terms_final_lemmatized = []
else:
    # Lematizar los términos ingresados por el usuario para el análisis
    landscape_terms_final_lemmatized = [lemmatizer.lemmatize(term, get_wordnet_pos(nltk.pos_tag([term])[0][1])) for term in landscape_terms_from_user_input]
    # Alternativa más simple para lematizar si no se quiere re-POS-tagear cada término ingresado:
    # landscape_terms_final_lemmatized = [lemmatizer.lemmatize(term, wordnet.NOUN) for term in landscape_terms_from_user_input] # Asumir sustantivo por defecto
    print(f"\nSe analizarán los siguientes términos (lematizados): {landscape_terms_final_lemmatized}")
print("="*50 + "\n")

# --- PASO 5: CONTEO DE FRECUENCIA DE TÉRMINOS PAISAJÍSTICOS SELECCIONADOS (LEMATIZADOS) ---
print("\n--- Iniciando Paso 5: Análisis de Frecuencia de Términos Seleccionados ---")
landscape_term_counts = Counter()
# Contar sobre los tokens lematizados del texto principal
for token_lem in tokens_lemmatized:
    if token_lem in landscape_terms_final_lemmatized:
        landscape_term_counts[token_lem] += 1

print("\n--- Frecuencia de Términos Paisajísticos Seleccionados (Lematizados) ---")
if landscape_term_counts:
    for term, count in landscape_term_counts.most_common():
        print(f"{term}: {count}")
else:
    if landscape_terms_final_lemmatized:
        print("Ninguno de los términos seleccionados (lematizados) fue encontrado en el texto lematizado.")
    else:
        print("No se seleccionaron términos para analizar su frecuencia específica.")

# --- PASO 6: ANÁLISIS DE CONCORDANCIAS DE TÉRMINOS SELECCIONADOS (LEMATIZADOS) ---
print("\n--- Iniciando Paso 6: Análisis de Concordancias de Términos Seleccionados ---")
if landscape_terms_final_lemmatized:
    print(f"(Nota: Las concordancias se mostrarán sobre el texto lematizado ya que nltk_text usa tokens_lemmatized)")
    for term_lem in landscape_terms_final_lemmatized:
        if term_lem in nltk_text.tokens: # nltk_text.tokens son tokens_lemmatized
            print(f"\nConcordancias para '{term_lem}':")
            nltk_text.concordance(term_lem, width=80, lines=10)
        else:
            ### MODIFICACIÓN: Comentario aclarado
            print(f"\nEl término '{term_lem}' (lematizado) no se encontró en el texto lematizado (nltk_text.tokens) para concordancia.")
else:
    print("No se seleccionaron términos para mostrar concordancias.")