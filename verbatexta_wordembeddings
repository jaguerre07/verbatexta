import nltk
# --- NLTK Downloads: Ejecutar una vez en consola de Python si es necesario, luego comentar ---
# nltk.download('punkt')
# # nltk.download('punkt_tab') # 'punkt' es el principal
# nltk.download('stopwords')
# nltk.download('averaged_perceptron_tagger')
# # nltk.download('averaged_perceptron_tagger_eng') # 'averaged_perceptron_tagger' es suficiente
# nltk.download('wordnet')
# nltk.download('omw-1.4') # Necesario para WordNetLemmatizer en algunos idiomas/versiones

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.text import Text # Para concordancias
import string # Para manejar puntuación
from collections import Counter
from nltk.corpus import wordnet
from gensim.models import Word2Vec # Importar Word2Vec
# import os # Si fueras a usar variables de entorno para claves API

# --- Función para mapear etiquetas POS de NLTK a WordNet ---
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Por defecto, sustantivo

# --- PASO 1: CARGAR EL ARCHIVO DE TEXTO ---
file_path = '/Users/juanaguerre/Downloads/purple_land_hudson.txt'
try:
    with open(file_path, 'r', encoding='utf-8') as file:
        raw_text = file.read()
except FileNotFoundError:
    print(f"Error: El archivo '{file_path}' no fue encontrado. Verifica la ruta.")
    exit()

# --- PASO 2: PREPROCESAMIENTO BÁSICO GENERAL ---
print("\n--- Iniciando Paso 2: Preprocesamiento Básico General ---")
tokens_originales = word_tokenize(raw_text)
tokens_lower = [token.lower() for token in tokens_originales]

tokens_no_punct_list = [token for token in tokens_lower if token.isalpha()]
stop_words_english = set(stopwords.words('english'))
tokens_processed = [token for token in tokens_no_punct_list if token not in stop_words_english] # Usado para POS Tagging en Paso 3

lemmatizer = WordNetLemmatizer()
# Lematización: Primero POS tag sobre tokens_processed, luego lematizar.
pos_tags_for_lemmatization = nltk.pos_tag(tokens_processed) ### MODIFICACIÓN: POS Tagging sobre tokens_processed
tokens_lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags_for_lemmatization]

# nltk_text para concordancias se basará en tokens_lemmatized
nltk_text = Text(tokens_lemmatized)

print(f"Número de tokens originales: {len(tokens_originales)}")
print(f"Número de tokens procesados (antes de lematizar, para referencia): {len(tokens_processed)}")
print(f"Número de tokens lematizados (para análisis principal): {len(tokens_lemmatized)}")
print("\nPrimeros 20 tokens lematizados:")
print(tokens_lemmatized[:20])

# --- PASO 2.1: CONTEO DE FRECUENCIA DE TODAS LAS PALABRAS LEMATIZADAS ---
print("\n--- Iniciando Paso 2.1: Frecuencia de Todas las Palabras Lematizadas ---")
word_counts = Counter(tokens_lemmatized) # Usar tokens lematizados para el conteo general
print("\n--- Frecuencia de Todas las Palabras Lematizadas (Top 20) ---")
for word, count in word_counts.most_common(20):
    print(f"{word}: {count}")

# --- PASO 3: GENERACIÓN DE CANDIDATOS DE TÉRMINOS (con POS Tagging, antes de lematizar) ---
print("\n--- Iniciando Paso 3: Generación de Candidatos con POS Tagging ---")
### MODIFICACIÓN: Aplicar POS Tagging sobre 'tokens_processed' (sin stopwords/puntuación, pero NO lematizados aún)
### Esto es para obtener candidatos en su forma más natural antes de que el usuario los seleccione y se lematicen.
tagged_tokens_for_candidates = nltk.pos_tag(tokens_processed)

pos_tags_of_interest = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']
candidate_terms_from_pos = []
for word, tag in tagged_tokens_for_candidates:
    if tag in pos_tags_of_interest:
        # word.isalpha() ya está implícito porque tokens_processed se basa en tokens_no_punct_list
        # stopwords ya fueron removidas para crear tokens_processed
        candidate_terms_from_pos.append(word)

candidate_counts = Counter(candidate_terms_from_pos)

print("\n--- Lista de Candidatos por POS Tagging (Top 50 más frecuentes, formas originales) ---")
print("REVISA ESTA LISTA. Los términos que elijas se lematizarán después.")
for term, count in candidate_counts.most_common(50):
    print(f"{term}: {count}")

# --- PASO 3.5: FILTRADO SEMÁNTICO LOCAL CON WORD EMBEDDINGS (GENSIM) ---
print("\n" + "="*50)
print("PASO 3.5: FILTRADO SEMÁNTICO LOCAL CON WORD EMBEDDINGS (GENSIM)")
print("="*50)

print("Preparando datos para el modelo Word2Vec...")
raw_sentences = sent_tokenize(raw_text.lower())
word2vec_corpus = [word_tokenize(sentence) for sentence in raw_sentences]
word2vec_corpus_cleaned = []
for sentence_tokens in word2vec_corpus:
    cleaned_sentence = [token for token in sentence_tokens if token.isalpha()]
    if cleaned_sentence:
        word2vec_corpus_cleaned.append(cleaned_sentence)

semantic_suggestions = [] # Inicializar por si el corpus está vacío
if not word2vec_corpus_cleaned:
    print("Error: El corpus para Word2Vec está vacío después de la limpieza. Se omite el filtrado semántico con Word2Vec.")
else:
    vector_dim = 100
    window_size = 5
    min_word_count = 3
    workers_count = 4
    sg_param = 0
    print(f"Entrenando modelo Word2Vec (vector_size={vector_dim}, window={window_size}, min_count={min_word_count}, sg={sg_param})...")
    word2vec_model = Word2Vec(sentences=word2vec_corpus_cleaned,
                              vector_size=vector_dim, window=window_size,
                              min_count=min_word_count, workers=workers_count, sg=sg_param)
    print("Modelo Word2Vec entrenado.")
      # 3. Definir/Ingresar palabras semilla para "paisaje y naturaleza"
    default_seed_words_nature = [
        'tree', 'river', 'mountain', 'forest', 'pampa', 'sky', 'water', 'plain',
        'grass', 'stone', 'wind', 'flower', 'sea', 'hill', 'valley', 'sun',
        'moon', 'stars', 'animal', 'bird', 'wood', 'field', 'soil', 'plant',
        'nature', 'landscape', 'ground', 'earth'
    ]
    print("\nPalabras semilla por defecto para 'paisaje/naturaleza':")
    print(", ".join(default_seed_words_nature))

    use_default_seeds_prompt = input("¿Usar esta lista de semillas por defecto? (s/n, 's' por defecto): ").strip().lower()

    seed_words_nature_final = []
    if use_default_seeds_prompt == 'n':
        print("\nIngresa tus propias palabras semilla para 'paisaje/naturaleza', separadas por comas.")
        user_seed_input_string = input("Tus palabras semilla: ").strip()
        if user_seed_input_string:
            raw_seed_terms = user_seed_input_string.split(',')
            for term in raw_seed_terms:
                cleaned_term = term.strip().lower()
                if cleaned_term:
                    seed_words_nature_final.append(cleaned_term)
        if not seed_words_nature_final: # Si el usuario ingresó 'n' pero no dio palabras
            print("No se ingresaron palabras semilla personalizadas. Usando la lista por defecto.")
            seed_words_nature_final = default_seed_words_nature
    else: # Si el usuario presiona Enter o escribe 's' (o cualquier cosa que no sea 'n')
        seed_words_nature_final = default_seed_words_nature

    print(f"Se usarán las siguientes palabras semilla para el análisis de similitud: {seed_words_nature_final}")

    # Filtrar seed_words_nature_final para que solo contengan palabras que están en el vocabulario del modelo
    valid_seed_words = [word for word in seed_words_nature_final if word in word2vec_model.wv]

    if not valid_seed_words:
        print("Advertencia: Ninguna de las palabras semilla seleccionadas se encuentra en el vocabulario del modelo Word2Vec.")
        semantic_suggestions = []
    else:
        print(f"Palabras semilla válidas en el modelo para 'paisaje/naturaleza': {valid_seed_words}")
        temp_counter_for_candidates = Counter()
        for sentence in word2vec_corpus_cleaned: # Usar el mismo corpus con el que se entrenó WV
            temp_counter_for_candidates.update(sentence)
        candidate_terms_for_wv = [word for word, count in temp_counter_for_candidates.most_common(500)]
        similarity_threshold = 0.55 # Experimenta con este valor
        print(f"\nFiltrando ~{len(candidate_terms_for_wv)} términos (formas originales) con umbral > {similarity_threshold}...")
        for term in candidate_terms_for_wv:
            if term in word2vec_model.wv:
                try:
                    max_similarity = 0.0
                    for seed in valid_seed_words:
                        sim = word2vec_model.wv.similarity(term, seed)
                        if sim > max_similarity:
                            max_similarity = sim
                    if max_similarity > similarity_threshold:
                        if term not in valid_seed_words:
                            semantic_suggestions.append(term)
                except KeyError:
                    pass
        semantic_suggestions = sorted(list(dict.fromkeys(semantic_suggestions)))

print("\nTérminos sugeridos por Word2Vec local (similitud con palabras semilla):")
if semantic_suggestions:
    print(", ".join(semantic_suggestions))
else:
    print("No se encontraron sugerencias semánticas con los parámetros actuales (o no había palabras semilla válidas).")

# --- PASO 4: ENTRADA INTERACTIVA DEL USUARIO PARA SELECCIONAR TÉRMINOS ---
print("\n" + "="*50)
print("PASO 4: SELECCIÓN INTERACTIVA DE TÉRMINOS")
print("="*50)
### MODIFICACIÓN: Actualizar el prompt para el usuario
print("Por favor, revisa la 'Lista de Candidatos por POS Tagging' (Paso 3) y los")
print("'Términos sugeridos por Word2Vec' (Paso 3.5) impresos arriba.")
print("Escribe los términos que consideres relevantes para tu análisis (se lematizarán después).")
input_prompt = "Ingresa los términos separados por comas (ej: pampa, monte, land, sky): "
user_input_string = input(input_prompt)

landscape_terms_from_user_input = []
if user_input_string.strip():
    raw_terms = user_input_string.split(',')
    for term in raw_terms:
        cleaned_term = term.strip().lower()
        if cleaned_term:
            landscape_terms_from_user_input.append(cleaned_term)

if not landscape_terms_from_user_input:
    print("No se ingresaron términos. El análisis de términos específicos se omitirá.")
    landscape_terms_final_lemmatized = []
else:
    # Lematizar los términos ingresados por el usuario para el análisis
    landscape_terms_final_lemmatized = [lemmatizer.lemmatize(term, get_wordnet_pos(nltk.pos_tag([term])[0][1])) for term in landscape_terms_from_user_input]
    # Alternativa más simple para lematizar si no se quiere re-POS-tagear cada término ingresado:
    # landscape_terms_final_lemmatized = [lemmatizer.lemmatize(term, wordnet.NOUN) for term in landscape_terms_from_user_input] # Asumir sustantivo por defecto
    print(f"\nSe analizarán los siguientes términos (lematizados): {landscape_terms_final_lemmatized}")
print("="*50 + "\n")

# --- PASO 5: CONTEO DE FRECUENCIA DE TÉRMINOS PAISAJÍSTICOS SELECCIONADOS (LEMATIZADOS) ---
print("\n--- Iniciando Paso 5: Análisis de Frecuencia de Términos Seleccionados ---")
landscape_term_counts = Counter()
# Contar sobre los tokens lematizados del texto principal
for token_lem in tokens_lemmatized:
    if token_lem in landscape_terms_final_lemmatized:
        landscape_term_counts[token_lem] += 1

print("\n--- Frecuencia de Términos Paisajísticos Seleccionados (Lematizados) ---")
if landscape_term_counts:
    for term, count in landscape_term_counts.most_common():
        print(f"{term}: {count}")
else:
    if landscape_terms_final_lemmatized:
        print("Ninguno de los términos seleccionados (lematizados) fue encontrado en el texto lematizado.")
    else:
        print("No se seleccionaron términos para analizar su frecuencia específica.")

# --- PASO 6: ANÁLISIS DE CONCORDANCIAS DE TÉRMINOS SELECCIONADOS (LEMATIZADOS) ---
print("\n--- Iniciando Paso 6: Análisis de Concordancias de Términos Seleccionados ---")
if landscape_terms_final_lemmatized:
    print(f"(Nota: Las concordancias se mostrarán sobre el texto lematizado ya que nltk_text usa tokens_lemmatized)")
    for term_lem in landscape_terms_final_lemmatized:
        if term_lem in nltk_text.tokens: # nltk_text.tokens son tokens_lemmatized
            print(f"\nConcordancias para '{term_lem}':")
            nltk_text.concordance(term_lem, width=80, lines=10)
        else:
            ### MODIFICACIÓN: Comentario aclarado
            print(f"\nEl término '{term_lem}' (lematizado) no se encontró en el texto lematizado (nltk_text.tokens) para concordancia.")
else:
    print("No se seleccionaron términos para mostrar concordancias.")